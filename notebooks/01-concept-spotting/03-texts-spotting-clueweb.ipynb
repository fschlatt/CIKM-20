{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.models import SequenceTagger\n",
    "from flair.data import Sentence, Token\n",
    "import stanfordnlp\n",
    "import json\n",
    "import tqdm\n",
    "import numpy as np\n",
    "from itertools import repeat, takewhile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CLUEWEB_EXTRACTION = \"/mnt/ceph/storage/data-in-progress/data-research/web-search/health-question-answering/causenet-data/causality-graphs/extraction/\"\n",
    "PATH_CLUEWEB_EXTRACTION += \"clueweb12/clueweb12-extraction.tsv\"\n",
    "PATH_FLAIR_FOLDER = \"/mnt/ceph/storage/data-in-progress/data-research/web-search/health-question-answering/causenet-data/flair-models/texts/\"\n",
    "\n",
    "PATH_STANFORD_RESOURCES = \"/mnt/ceph/storage/data-in-progress/data-research/web-search/health-question-answering/causenet-data/external/stanfordnlp/\"\n",
    "\n",
    "PATH_OUTPUT_GRAPH = \"/mnt/ceph/storage/data-in-progress/data-research/web-search/health-question-answering/causenet-data/causality-graphs/spotting/\"\n",
    "PATH_OUTPUT_GRAPH += \"clueweb12/clueweb-graph.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_lines(file, verbose=False):\n",
    "    bufgen = takewhile(lambda x: x, (file.read(1024 * 1024) for _ in repeat(None)))\n",
    "    count = 0\n",
    "    for buf in bufgen:\n",
    "        count += buf.count(b\"\\n\")\n",
    "        if verbose:\n",
    "            print(count, end=\"\\r\")\n",
    "    if verbose:\n",
    "        print()\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanfordnlp.download('en', PATH_STANFORD_RESOURCES)\n",
    "stanford_nlp = stanfordnlp.Pipeline(processors='tokenize,pos',\n",
    "                                    tokenize_pretokenized=True,\n",
    "                                    models_dir=PATH_STANFORD_RESOURCES,\n",
    "                                    treebank='en_ewt',\n",
    "                                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading sentence data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence_set = []\n",
    "\n",
    "with open(PATH_CLUEWEB_EXTRACTION, \"rb\") as file:\n",
    "    num_lines = count_lines(file)\n",
    "\n",
    "with open(PATH_CLUEWEB_EXTRACTION, encoding=\"utf-8\") as file:\n",
    "    for line in tqdm.tqdm(file, total=num_lines):\n",
    "        parts = line.strip().split('\\t')\n",
    "        if parts[0] != 'clueweb12_sentence':\n",
    "            continue\n",
    "        assert len(parts) == 8\n",
    "        \n",
    "        for match in json.loads(parts[7]):\n",
    "            sentence_data = {\n",
    "                \"causal_relation\": match,\n",
    "                \"sources\": [{\n",
    "                    \"type\": \"clueweb12_sentence\",\n",
    "                    \"payload\": {\n",
    "                        \"clueweb12_page_id\": parts[1],\n",
    "                        \"clueweb12_page_reference\": parts[2],\n",
    "                        \"clueweb12_page_timestamp\": parts[3],\n",
    "                        \"sentence\": {\n",
    "                            \"surface\": json.loads(parts[4]),\n",
    "                            \"tokens\": json.loads(parts[5]),\n",
    "                            \"dependencies\": json.loads(parts[6])\n",
    "                            },\n",
    "                        \"path_pattern\": match['Pattern']\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            sentence_set.append(sentence_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offset_of_tags_in_sentence(sentence, tags):\n",
    "    # go from left to right and determine tag offsets\n",
    "    offsets = []\n",
    "    total_offset = 0\n",
    "    for tag in tags:\n",
    "        label = tag[0]\n",
    "        local_offset = sentence.find(label)\n",
    "        offset = total_offset + local_offset\n",
    "        offsets.append(offset)\n",
    "\n",
    "        # prepare for next iteration\n",
    "        sentence = sentence[local_offset + len(label):]\n",
    "        total_offset = offset + len(label)\n",
    "    return offsets\n",
    "\n",
    "\n",
    "def get_pos_tags_of_sentence(sentence):\n",
    "    tags = []\n",
    "    for token in sentence.tokens:\n",
    "        for word in token.words:\n",
    "            tags.append((word.text, word.pos))\n",
    "    return tags\n",
    "\n",
    "\n",
    "def calculate_pos_tags_for_string(doc):\n",
    "    tags = []\n",
    "    for sentence in doc.sentences:\n",
    "        sentence_pos = []\n",
    "        for pos in get_pos_tags_of_sentence(sentence):\n",
    "            sentence_pos.append(pos)\n",
    "        tags.append(sentence_pos)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagging(sentences_to_predict):\n",
    "    batch_of_tokens = [sample['sources'][0]['payload']['sentence']['tokens']\n",
    "                       for sample in sentences_to_predict]\n",
    "    strings = [' '.join(tokens) for tokens in batch_of_tokens]\n",
    "\n",
    "    # batch processing is faster\n",
    "    batch = '\\n\\n'.join(strings)\n",
    "    doc = stanford_nlp(batch)\n",
    "    tags = calculate_pos_tags_for_string(doc)\n",
    "\n",
    "    assert len(tags) == len(sentences_to_predict)\n",
    "\n",
    "    for i in range(len(sentences_to_predict)):\n",
    "        sample = sentences_to_predict[i]\n",
    "        sentence = sample['sources'][0]['payload']['sentence']['surface']\n",
    "        offsets = get_offset_of_tags_in_sentence(sentence, tags[i])\n",
    "        sample['sources'][0]['payload']['sentence']['POS'] = [\n",
    "            (tags[i][x][0], tags[i][x][1], str(offsets[x]))\n",
    "            for x in range(len(tags[i]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-Spotter: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(batch):\n",
    "    sentences = []\n",
    "\n",
    "    for sample in batch:\n",
    "        sentence = Sentence(use_tokenizer=False)\n",
    "\n",
    "        tokens = sample['sources'][0]['payload']['sentence']['tokens']\n",
    "        POS_tags = sample['sources'][0]['payload']['sentence']['POS']\n",
    "        if len(tokens) > 200:\n",
    "            # skipping sentences with too many tokens\n",
    "            # due to GPU memory limitation\n",
    "            continue\n",
    "\n",
    "        for pos in POS_tags:\n",
    "            token = Token(pos[0])\n",
    "            token.add_tag('POS', pos[1])\n",
    "            token.add_tag('idx', pos[2])\n",
    "            sentence.add_token(token)\n",
    "\n",
    "        sentences.append(sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentences, mini_batches):\n",
    "    prediction = []\n",
    "    classifier.predict(sentences, mini_batches)\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        sentence = sentences[i]\n",
    "        indices = [[token.idx-1 for token in chunk.tokens]\n",
    "                   for chunk in sentence.get_spans('chunk_BIO')]\n",
    "\n",
    "        extraction = []\n",
    "        for index_list in indices:\n",
    "            result = [sentence.tokens[index].text\n",
    "                      for index in index_list]\n",
    "            extraction.append(' '.join(result))\n",
    "\n",
    "        prediction.append([extraction, indices])\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_match(relation, indices):\n",
    "    cause_index = int(relation['Cause'][1])\n",
    "    effect_index = int(relation['Effect'][1])\n",
    "\n",
    "    cause_match = None\n",
    "    effect_match = None\n",
    "\n",
    "    for index_range in indices:\n",
    "        if cause_index in index_range:\n",
    "            cause_match = indices.index(index_range)\n",
    "        if effect_index in index_range:\n",
    "            effect_match = indices.index(index_range)\n",
    "\n",
    "        if (cause_match is not None\n",
    "                and effect_match is not None\n",
    "                and cause_match != effect_match):\n",
    "            return [cause_match, effect_match]\n",
    "    return []\n",
    "\n",
    "\n",
    "def get_relations(batch, prediction):\n",
    "    relations = []\n",
    "    skipped_elements = 0\n",
    "\n",
    "    for i in range(len(batch)):\n",
    "        sample = batch[i]\n",
    "        tokens = sample['sources'][0]['payload']['sentence']['tokens']\n",
    "        POS_tags = sample['sources'][0]['payload']['sentence']['POS']\n",
    "        if len(tokens) > 200:\n",
    "            # skipping sentences with too many tokens\n",
    "            # due to GPU memory limitation\n",
    "            # see method prepare(batch)\n",
    "            skipped_elements += 1\n",
    "            continue\n",
    "\n",
    "        path_pattern_extraction = sample['causal_relation']\n",
    "        spotting_extraction, indices = prediction[i - skipped_elements]\n",
    "\n",
    "        match = find_match(path_pattern_extraction, indices)\n",
    "        if len(match) < 2:\n",
    "            # In cases the tagger failed,\n",
    "            # we disregarded the causal concepts\n",
    "            continue\n",
    "        cause_match, effect_match = match\n",
    "\n",
    "        cause = spotting_extraction[cause_match]\n",
    "        effect = spotting_extraction[effect_match]\n",
    "\n",
    "        # concept POS (save for later post-processing)\n",
    "        cause_pos_raw = [POS_tags[j] for j in indices[cause_match]]\n",
    "        offset = get_offset_of_tags_in_sentence(cause, cause_pos_raw)\n",
    "        cause_pos = [(cause_pos_raw[x][0],\n",
    "                      cause_pos_raw[x][1],\n",
    "                      str(offset[x]))\n",
    "                     for x in range(len(cause_pos_raw))]\n",
    "\n",
    "        effect_pos_raw = [POS_tags[j] for j in indices[effect_match]]\n",
    "        offset = get_offset_of_tags_in_sentence(effect, effect_pos_raw)\n",
    "        effect_pos = [(effect_pos_raw[x][0],\n",
    "                       effect_pos_raw[x][1],\n",
    "                       str(offset[x]))\n",
    "                      for x in range(len(effect_pos_raw))]\n",
    "\n",
    "        causal_relation = {'causal_relation': {\n",
    "            'cause': {'concept': cause, 'POS': cause_pos, 'idcs': indices[cause_match]},\n",
    "            'effect': {'concept': effect, 'POS': effect_pos, 'idcs': indices[effect_match]},\n",
    "        }, 'sources': sample['sources']}\n",
    "        relations.append(causal_relation)\n",
    "    return relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SequenceTagger.load(PATH_FLAIR_FOLDER + 'final-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_graph = []\n",
    "\n",
    "batch_size = 512 if len(sentence_set) > 512 else 32\n",
    "batches = np.array_split(sentence_set, len(sentence_set)/batch_size)\n",
    "\n",
    "for i in tqdm.tqdm(range(len(batches))):\n",
    "    batch = batches[i]\n",
    "    pos_tagging(batch)\n",
    "    prepared_sentences = prepare(batch)\n",
    "    prediction = predict(prepared_sentences, mini_batches=batch_size)\n",
    "    batch_relations = get_relations(batch, prediction)\n",
    "\n",
    "    text_graph.extend(batch_relations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_idcs(tokens, sentence):\n",
    "    idcs = []\n",
    "    idx = 0\n",
    "    for token in tokens:\n",
    "        token_idx = sentence[idx:].find(token)\n",
    "        idcs.append((token_idx + idx, token_idx + len(token) + idx))\n",
    "        idx = idcs[-1][1]\n",
    "    return idcs\n",
    "\n",
    "def post_process(pos_tags, tokens, indices):\n",
    "    left = 0\n",
    "    right = len(pos_tags)\n",
    "\n",
    "    punctuation = ['.', ',', ';', '(', ')', '``', \"''\"]\n",
    "    cutoff = ['CC', 'DT', 'PRP', 'PRP$'] + punctuation\n",
    "\n",
    "    for tag in pos_tags:\n",
    "        if tag[1] in cutoff:\n",
    "            left += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    for tag in reversed(pos_tags):\n",
    "        if tag[1] in cutoff:\n",
    "            right -= 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    indices = indices[left:right]\n",
    "    value = \" \".join(tokens[idx] for idx in indices)\n",
    "\n",
    "    return value, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for relation in tqdm.tqdm(text_graph):\n",
    "    tokens = relation['sources'][0]['payload']['sentence']['tokens']\n",
    "    sentence = relation['sources'][0]['payload']['sentence']['surface']\n",
    "    token_idcs = get_token_idcs(tokens, sentence)\n",
    "    cause_token_idcs = relation['causal_relation']['cause']['idcs']\n",
    "    effect_token_idcs = relation['causal_relation']['effect']['idcs']\n",
    "\n",
    "    cause_concept = relation['causal_relation']['cause']['concept']\n",
    "    cause_pos = relation['causal_relation']['cause']['POS']\n",
    "    cause, cause_token_idcs = post_process(cause_pos, tokens, cause_token_idcs)\n",
    "\n",
    "    effect_concept = relation['causal_relation']['effect']['concept']\n",
    "    effect_pos = relation['causal_relation']['effect']['POS']\n",
    "    effect, effect_token_idcs = post_process(effect_pos, tokens, effect_token_idcs)\n",
    "\n",
    "    cause_idcs = [token_idcs[cause_token_idcs[0]][0], token_idcs[cause_token_idcs[-1]][1]]\n",
    "    effect_idcs = [token_idcs[effect_token_idcs[0]][0], token_idcs[effect_token_idcs[-1]][1]]\n",
    "\n",
    "    causal_relation = {\n",
    "       'cause': {'concept': cause},\n",
    "        'effect': {'concept': effect}\n",
    "    }\n",
    "    relation['causal_relation'] = causal_relation\n",
    "    relation['sources'][0]['payload']['idcs'] = {\n",
    "        'surface': {'cause': cause_idcs, 'effect': effect_idcs},\n",
    "        'tokens': {'cause': cause_token_idcs, 'effect': effect_token_idcs}\n",
    "    }\n",
    "\n",
    "    # further cleanup\n",
    "    del relation['sources'][0]['payload']['sentence']['POS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Text-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonarray = json.dumps(text_graph)\n",
    "with open(PATH_OUTPUT_GRAPH, \"w+\") as file:\n",
    "    file.write(jsonarray)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c29631e0fe057d8616f51265d6a56ef9ab0b314d1ae1549f5d4a17b7e429073c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('cikm20-causenet': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
